{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d07acdb-579e-427f-9de8-7ee63de7acb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', \"It's\", 'a', 'sunny', 'day,', 'and', 'everyone', 'is', 'happy!', \"Let's\", 'go', 'outside.']\n",
      "\n",
      "Manual tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'sunny', 'day', ',', 'and', 'everyone', 'is', 'happy', '!', 'Let', \"'s\", 'go', 'outside', '.']\n",
      "\n",
      "NLTK tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'sunny', 'day', ',', 'and', 'everyone', 'is', 'happy', '!', 'Let', \"'s\", 'go', 'outside', '.']\n",
      "\n",
      "Differences (Manual vs NLTK): []\n",
      "\n",
      "Multiword Expressions: [('New York', 'Place name'), ('kick the bucket', 'Idiom'), ('machine learning', 'Technical phrase')]\n",
      "\n",
      "Reflection:\n",
      " Hardest part was punctuation and contractions. English clitics split differently by tools. Morphology in other languages adds complexity. Naive split merges punctuation with words. MWEs are tricky since tools split them but meaning requires them whole. Tokenization needs linguistic awareness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download tokenizer data if not already available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1. Tokenize paragraph\n",
    "paragraph = \"The quick brown fox jumps over the lazy dog. It's a sunny day, and everyone is happy! Let's go outside.\"\n",
    "naive_tokens = paragraph.split()\n",
    "manual_tokens = [\n",
    "    \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\",\n",
    "    \"It\", \"'s\", \"a\", \"sunny\", \"day\", \",\", \"and\", \"everyone\", \"is\", \"happy\", \"!\",\n",
    "    \"Let\", \"'s\", \"go\", \"outside\", \".\"\n",
    "]\n",
    "\n",
    "# 2. Compare with NLTK\n",
    "tool_tokens_nltk = nltk.word_tokenize(paragraph)\n",
    "\n",
    "def compare_tokens(manual, tool):\n",
    "    return [(m, t) for m, t in zip(manual, tool) if m != t]\n",
    "\n",
    "nltk_diff = compare_tokens(manual_tokens, tool_tokens_nltk)\n",
    "\n",
    "# 3. MWEs\n",
    "mwes = [\n",
    "    (\"New York\", \"Place name\"),\n",
    "    (\"kick the bucket\", \"Idiom\"),\n",
    "    (\"machine learning\", \"Technical phrase\")\n",
    "]\n",
    "\n",
    "# 4. Reflection\n",
    "reflection = (\n",
    "    \"Hardest part was punctuation and contractions. \"\n",
    "    \"English clitics split differently by tools. \"\n",
    "    \"Morphology in other languages adds complexity. \"\n",
    "    \"Naive split merges punctuation with words. \"\n",
    "    \"MWEs are tricky since tools split them but meaning requires them whole. \"\n",
    "    \"Tokenization needs linguistic awareness.\"\n",
    ")\n",
    "\n",
    "# Output\n",
    "print(\"Naive tokens:\", naive_tokens)\n",
    "print(\"\\nManual tokens:\", manual_tokens)\n",
    "print(\"\\nNLTK tokens:\", tool_tokens_nltk)\n",
    "print(\"\\nDifferences (Manual vs NLTK):\", nltk_diff)\n",
    "print(\"\\nMultiword Expressions:\", mwes)\n",
    "print(\"\\nReflection:\\n\", reflection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f62601-91c8-4fe1-8e87-869f53e311cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('e', 'r') (freq=9) -> 'er' | Vocab size: 12\n",
      "Merge 2: ('er', '_') (freq=9) -> 'er_' | Vocab size: 13\n",
      "Merge 3: ('n', 'e') (freq=8) -> 'ne' | Vocab size: 14\n",
      "Merge 4: ('ne', 'w') (freq=8) -> 'new' | Vocab size: 15\n",
      "Merge 5: ('l', 'o') (freq=7) -> 'lo' | Vocab size: 16\n",
      "Merge 6: ('lo', 'w') (freq=7) -> 'low' | Vocab size: 17\n",
      "Merge 7: ('new', 'er_') (freq=6) -> 'newer_' | Vocab size: 18\n",
      "Merge 8: ('low', '_') (freq=5) -> 'low_' | Vocab size: 19\n",
      "Merge 9: ('w', 'i') (freq=3) -> 'wi' | Vocab size: 20\n",
      "Merge 10: ('wi', 'd') (freq=3) -> 'wid' | Vocab size: 21\n",
      "Learned merges: [(('e', 'r'), 9, 'er'), (('er', '_'), 9, 'er_'), (('n', 'e'), 8, 'ne'), (('ne', 'w'), 8, 'new'), (('l', 'o'), 7, 'lo'), (('lo', 'w'), 7, 'low'), (('new', 'er_'), 6, 'newer_'), (('low', '_'), 5, 'low_'), (('w', 'i'), 3, 'wi'), (('wi', 'd'), 3, 'wid')]\n",
      "new -> ['new', '_']\n",
      "newer -> ['newer_']\n",
      "lowest -> ['low', 'e', 's', 't', '_']\n",
      "widest -> ['wid', 'e', 's', 't', '_']\n",
      "newestest -> ['new', 'e', 's', 't', 'e', 's', 't', '_']\n"
     ]
    }
   ],
   "source": [
    "# mini_bpe.py\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_word_tokens(corpus_words):\n",
    "    return [tuple(list(w) + ['_']) for w in corpus_words]\n",
    "\n",
    "def get_pair_counts(word_tokens):\n",
    "    counts = Counter()\n",
    "    for wt in word_tokens:\n",
    "        for i in range(len(wt)-1):\n",
    "            counts[(wt[i], wt[i+1])] += 1\n",
    "    return counts\n",
    "\n",
    "def merge_pair_in_tokens(word_tokens, pair):\n",
    "    a,b = pair\n",
    "    merged = a+b\n",
    "    new_tokens = []\n",
    "    for wt in word_tokens:\n",
    "        new_w = []\n",
    "        i=0\n",
    "        while i < len(wt):\n",
    "            if i < len(wt)-1 and wt[i]==a and wt[i+1]==b:\n",
    "                new_w.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_w.append(wt[i])\n",
    "                i += 1\n",
    "        new_tokens.append(tuple(new_w))\n",
    "    return new_tokens\n",
    "\n",
    "def learn_bpe(word_list, num_merges=10):\n",
    "    wtoks = get_word_tokens(word_list)\n",
    "    vocab = set(sorted(set(\"\".join(word_list)) | {'_'}))\n",
    "    merge_ops = []\n",
    "    for i in range(num_merges):\n",
    "        counts = get_pair_counts(wtoks)\n",
    "        if not counts:\n",
    "            break\n",
    "        top_pair, freq = counts.most_common(1)[0]\n",
    "        new_tok = top_pair[0] + top_pair[1]\n",
    "        merge_ops.append((top_pair, freq, new_tok))\n",
    "        vocab.add(new_tok)\n",
    "        wtoks = merge_pair_in_tokens(wtoks, top_pair)\n",
    "        print(f\"Merge {i+1}: {top_pair} (freq={freq}) -> '{new_tok}' | Vocab size: {len(vocab)}\")\n",
    "    return merge_ops, wtoks, vocab\n",
    "\n",
    "def apply_merges_list(word, merge_ops):\n",
    "    toks = tuple(list(word) + ['_'])\n",
    "    for pair,_,_ in merge_ops:\n",
    "        toks = merge_pair_in_tokens([toks], pair)[0]\n",
    "    return list(toks)\n",
    "\n",
    "# Example use:\n",
    "toy = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\".split()\n",
    "merge_ops, tokens, vocab = learn_bpe(toy, num_merges=10)\n",
    "print(\"Learned merges:\", [(p,f,n) for p,f,n in merge_ops])\n",
    "for w in [\"new\",\"newer\",\"lowest\",\"widest\",\"newestest\"]:\n",
    "    print(w, \"->\", apply_merges_list(w, merge_ops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ebdc0f-9c36-4fff-8970-d1ec7d096c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3.1 Manual BPE ===\n",
      "Corpus: ['low', 'low', 'low', 'low', 'low', 'lowest', 'lowest', 'newer', 'newer', 'newer', 'newer', 'newer', 'newer', 'wider', 'wider', 'wider', 'new', 'new']\n",
      "Initial vocabulary: ['_', 'd', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
      "\n",
      "Step 1: top pair ('e', 'r') freq=9 -> 'er'\n",
      "  before: ['low_', 'low_', 'low_', 'low_', 'low_', 'lowest_', 'lowest_', 'newer_']\n",
      "  after : ['low_', 'low_', 'low_', 'low_', 'low_', 'lowest_', 'lowest_', 'newer_']\n",
      "  vocab size: 12\n",
      "\n",
      "Step 2: top pair ('er', '_') freq=9 -> 'er_'\n",
      "  before: ['low_', 'low_', 'low_', 'low_', 'low_', 'lowest_', 'lowest_', 'newer_']\n",
      "  after : ['low_', 'low_', 'low_', 'low_', 'low_', 'lowest_', 'lowest_', 'newer_']\n",
      "  vocab size: 13\n",
      "\n",
      "Step 3: top pair ('n', 'e') freq=8 -> 'ne'\n",
      "  before: ['low_', 'low_', 'low_', 'low_', 'low_', 'lowest_', 'lowest_', 'newer_']\n",
      "  after : ['low_', 'low_', 'low_', 'low_', 'low_', 'lowest_', 'lowest_', 'newer_']\n",
      "  vocab size: 14\n",
      "\n",
      "=== 3.2 Mini-BPE learner ===\n",
      "Merge 1: ('e', 'r') (freq=9) -> 'er' | Vocab size: 12\n",
      "Merge 2: ('er', '_') (freq=9) -> 'er_' | Vocab size: 13\n",
      "Merge 3: ('n', 'e') (freq=8) -> 'ne' | Vocab size: 14\n",
      "Merge 4: ('ne', 'w') (freq=8) -> 'new' | Vocab size: 15\n",
      "Merge 5: ('l', 'o') (freq=7) -> 'lo' | Vocab size: 16\n",
      "Merge 6: ('lo', 'w') (freq=7) -> 'low' | Vocab size: 17\n",
      "Merge 7: ('new', 'er_') (freq=6) -> 'newer_' | Vocab size: 18\n",
      "Merge 8: ('low', '_') (freq=5) -> 'low_' | Vocab size: 19\n",
      "Merge 9: ('w', 'i') (freq=3) -> 'wi' | Vocab size: 20\n",
      "Merge 10: ('wi', 'd') (freq=3) -> 'wid' | Vocab size: 21\n",
      "\n",
      "Segmentations:\n",
      "new -> ['new', '_']\n",
      "newer -> ['newer_']\n",
      "lowest -> ['low', 'e', 's', 't', '_']\n",
      "widest -> ['wid', 'e', 's', 't', '_']\n",
      "newestest -> ['new', 'e', 's', 't', 'e', 's', 't', '_']\n",
      "\n",
      "Reflection (3.2):\n",
      "- Subwords solve OOV by breaking unseen words into known parts.\n",
      "- Example: 'er_' corresponds to a real morpheme (-er suffix).\n",
      "- This makes BPE useful for rare or new forms.\n",
      "\n",
      "=== 3.3 Paragraph BPE (30 merges) ===\n",
      "Training words: ['artificial', 'intelligence', 'is', 'changing', 'how', 'we', 'work', 'and', 'learn', 'researchers', 'build', 'models', 'that', 'can', 'generate', 'text', 'translate', 'languages', 'and', 'assist'] ...\n",
      "Merge 1: ('s', '_') (freq=12) -> 's_' | Vocab size: 26\n",
      "Merge 2: ('e', '_') (freq=10) -> 'e_' | Vocab size: 27\n",
      "Merge 3: ('a', 'n') (freq=9) -> 'an' | Vocab size: 28\n",
      "Merge 4: ('a', 't') (freq=9) -> 'at' | Vocab size: 29\n",
      "Merge 5: ('d', '_') (freq=7) -> 'd_' | Vocab size: 30\n",
      "Merge 6: ('e', 'r') (freq=7) -> 'er' | Vocab size: 31\n",
      "Merge 7: ('a', 'r') (freq=6) -> 'ar' | Vocab size: 32\n",
      "Merge 8: ('a', 'l') (freq=5) -> 'al' | Vocab size: 33\n",
      "Merge 9: ('o', 'r') (freq=5) -> 'or' | Vocab size: 34\n",
      "Merge 10: ('t', 'i') (freq=4) -> 'ti' | Vocab size: 35\n",
      "Merge 11: ('e', 'n') (freq=4) -> 'en' | Vocab size: 36\n",
      "Merge 12: ('w', 'or') (freq=4) -> 'wor' | Vocab size: 37\n",
      "Merge 13: ('an', 'd_') (freq=4) -> 'and_' | Vocab size: 38\n",
      "Merge 14: ('at', 'i') (freq=4) -> 'ati' | Vocab size: 39\n",
      "Merge 15: ('o', 'n') (freq=4) -> 'on' | Vocab size: 40\n",
      "Merge 16: ('i', 'n') (freq=3) -> 'in' | Vocab size: 41\n",
      "Merge 17: ('t', 'e') (freq=3) -> 'te' | Vocab size: 42\n",
      "Merge 18: ('c', 'h') (freq=3) -> 'ch' | Vocab size: 43\n",
      "Merge 19: ('h', 'o') (freq=3) -> 'ho' | Vocab size: 44\n",
      "Merge 20: ('s', 'e') (freq=3) -> 'se' | Vocab size: 45\n",
      "Merge 21: ('y', '_') (freq=3) -> 'y_' | Vocab size: 46\n",
      "Merge 22: ('l', '_') (freq=3) -> 'l_' | Vocab size: 47\n",
      "Merge 23: ('ati', 'on') (freq=3) -> 'ation' | Vocab size: 48\n",
      "Merge 24: ('i', 'c') (freq=2) -> 'ic' | Vocab size: 49\n",
      "Merge 25: ('i', 'al') (freq=2) -> 'ial' | Vocab size: 50\n",
      "Merge 26: ('g', 'en') (freq=2) -> 'gen' | Vocab size: 51\n",
      "Merge 27: ('c', 'e_') (freq=2) -> 'ce_' | Vocab size: 52\n",
      "Merge 28: ('an', 'g') (freq=2) -> 'ang' | Vocab size: 53\n",
      "Merge 29: ('in', 'g') (freq=2) -> 'ing' | Vocab size: 54\n",
      "Merge 30: ('ing', '_') (freq=2) -> 'ing_' | Vocab size: 55\n",
      "\n",
      "Top 5 merges:\n",
      "1. ('s', '_') -> s_\n",
      "2. ('e', '_') -> e_\n",
      "3. ('a', 'n') -> an\n",
      "4. ('a', 't') -> at\n",
      "5. ('d', '_') -> d_\n",
      "\n",
      "Five longest subword tokens:\n",
      "ation\n",
      "and_\n",
      "ing_\n",
      "ang\n",
      "ati\n",
      "\n",
      "Segmentations for sample words:\n",
      "intelligence -> ['in', 'te', 'l', 'l', 'i', 'gen', 'ce_']\n",
      "researchers -> ['r', 'e', 'se', 'ar', 'ch', 'er', 's_']\n",
      "subword -> ['s', 'u', 'b', 'wor', 'd_']\n",
      "variations -> ['v', 'ar', 'i', 'ation', 's_']\n",
      "practitioners -> ['p', 'r', 'a', 'c', 'ti', 'ti', 'on', 'er', 's_']\n",
      "\n",
      "Reflection (3.3):\n",
      "- Learned subwords include stems, suffixes (e.g., 'ation'), and frequent whole words.\n",
      "- Pros: handles rare/derived words, reduces vocab size.\n",
      "- Cons: sometimes splits inside morphemes; not always linguistically clean.\n",
      "- Tokenization difficulty depends on morphology and MWEs in the language.\n"
     ]
    }
   ],
   "source": [
    "# Q3: Manual & Mini BPE Implementation\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# --- Helper functions ---\n",
    "def get_word_tokens(corpus_words):\n",
    "    return [tuple(list(w) + ['_']) for w in corpus_words]\n",
    "\n",
    "def get_pair_counts(word_tokens):\n",
    "    counts = Counter()\n",
    "    for wt in word_tokens:\n",
    "        for i in range(len(wt)-1):\n",
    "            counts[(wt[i], wt[i+1])] += 1\n",
    "    return counts\n",
    "\n",
    "def merge_pair_in_tokens(word_tokens, pair):\n",
    "    a,b = pair\n",
    "    merged = a+b\n",
    "    new_tokens = []\n",
    "    for wt in word_tokens:\n",
    "        new_w = []\n",
    "        i=0\n",
    "        while i < len(wt):\n",
    "            if i < len(wt)-1 and wt[i]==a and wt[i+1]==b:\n",
    "                new_w.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_w.append(wt[i])\n",
    "                i += 1\n",
    "        new_tokens.append(tuple(new_w))\n",
    "    return new_tokens\n",
    "\n",
    "def learn_bpe(word_list, num_merges=10):\n",
    "    wtoks = get_word_tokens(word_list)\n",
    "    vocab = set(sorted(set(\"\".join(word_list)) | {'_'}))\n",
    "    merge_ops = []\n",
    "    for i in range(num_merges):\n",
    "        counts = get_pair_counts(wtoks)\n",
    "        if not counts: break\n",
    "        top_pair, freq = counts.most_common(1)[0]\n",
    "        new_tok = top_pair[0] + top_pair[1]\n",
    "        merge_ops.append((top_pair, freq, new_tok))\n",
    "        vocab.add(new_tok)\n",
    "        wtoks = merge_pair_in_tokens(wtoks, top_pair)\n",
    "        print(f\"Merge {i+1}: {top_pair} (freq={freq}) -> '{new_tok}' | Vocab size: {len(vocab)}\")\n",
    "    return merge_ops, wtoks, vocab\n",
    "\n",
    "def apply_merges_list(word, merge_ops):\n",
    "    toks = tuple(list(word) + ['_'])\n",
    "    for pair,_,_ in merge_ops:\n",
    "        toks = merge_pair_in_tokens([toks], pair)[0]\n",
    "    return list(toks)\n",
    "\n",
    "\n",
    "# --- 3.1 Manual BPE on toy corpus ---\n",
    "toy = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\".split()\n",
    "print(\"=== 3.1 Manual BPE ===\")\n",
    "print(\"Corpus:\", toy)\n",
    "\n",
    "tokens = get_word_tokens(toy)\n",
    "vocab = set(sorted(set(\"\".join(toy)) | {\"_\"}))\n",
    "print(\"Initial vocabulary:\", sorted(list(vocab)))\n",
    "\n",
    "for step in range(1,4):\n",
    "    counts = get_pair_counts(tokens)\n",
    "    pair, freq = counts.most_common(1)[0]\n",
    "    merged = pair[0]+pair[1]\n",
    "    print(f\"\\nStep {step}: top pair {pair} freq={freq} -> '{merged}'\")\n",
    "    before = [\"\".join(t) for t in tokens][:8]\n",
    "    tokens = merge_pair_in_tokens(tokens, pair)\n",
    "    after = [\"\".join(t) for t in tokens][:8]\n",
    "    vocab.add(merged)\n",
    "    print(\"  before:\", before)\n",
    "    print(\"  after :\", after)\n",
    "    print(\"  vocab size:\", len(vocab))\n",
    "\n",
    "\n",
    "# --- 3.2 Mini-BPE learner ---\n",
    "print(\"\\n=== 3.2 Mini-BPE learner ===\")\n",
    "merge_ops, final_tokens, final_vocab = learn_bpe(toy, num_merges=10)\n",
    "\n",
    "words_to_segment = [\"new\", \"newer\", \"lowest\", \"widest\", \"newestest\"]\n",
    "print(\"\\nSegmentations:\")\n",
    "for w in words_to_segment:\n",
    "    print(f\"{w} -> {apply_merges_list(w, merge_ops)}\")\n",
    "\n",
    "print(\"\\nReflection (3.2):\")\n",
    "print(\"- Subwords solve OOV by breaking unseen words into known parts.\")\n",
    "print(\"- Example: 'er_' corresponds to a real morpheme (-er suffix).\")\n",
    "print(\"- This makes BPE useful for rare or new forms.\")\n",
    "\n",
    "\n",
    "# --- 3.3 Train BPE on paragraph ---\n",
    "print(\"\\n=== 3.3 Paragraph BPE (30 merges) ===\")\n",
    "paragraph = (\n",
    "    \"Artificial intelligence is changing how we work and learn. \"\n",
    "    \"Researchers build models that can generate text, translate languages, and assist creativity. \"\n",
    "    \"Small datasets still teach useful patterns, especially when using subword methods. \"\n",
    "    \"Subword tokenization helps with rare words and morphological variations. \"\n",
    "    \"Practitioners tune merge operations to balance vocabulary size and coverage.\"\n",
    ")\n",
    "words = re.findall(r\"\\b\\w+\\b\", paragraph.lower())\n",
    "print(\"Training words:\", words[:20], \"...\")\n",
    "\n",
    "merge_ops_para, final_tokens_para, final_vocab_para = learn_bpe(words, num_merges=30)\n",
    "\n",
    "print(\"\\nTop 5 merges:\")\n",
    "for i,(pair,freq,newtok) in enumerate(merge_ops_para[:5],1):\n",
    "    print(f\"{i}. {pair} -> {newtok}\")\n",
    "\n",
    "sorted_vocab_by_len = sorted(final_vocab_para, key=lambda x: (-len(x), x))\n",
    "print(\"\\nFive longest subword tokens:\")\n",
    "for tok in sorted_vocab_by_len[:5]:\n",
    "    print(tok)\n",
    "\n",
    "words_sample = [\"intelligence\", \"researchers\", \"subword\", \"variations\", \"practitioners\"]\n",
    "print(\"\\nSegmentations for sample words:\")\n",
    "for w in words_sample:\n",
    "    print(f\"{w} -> {apply_merges_list(w, merge_ops_para)}\")\n",
    "\n",
    "print(\"\\nReflection (3.3):\")\n",
    "print(\"- Learned subwords include stems, suffixes (e.g., 'ation'), and frequent whole words.\")\n",
    "print(\"- Pros: handles rare/derived words, reduces vocab size.\")\n",
    "print(\"- Cons: sometimes splits inside morphemes; not always linguistically clean.\")\n",
    "print(\"- Tokenization difficulty depends on morphology and MWEs in the language.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a680a923-a060-4673-901d-43d06734884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model A: sub=1, ins=1, del=1 ===\n",
      "Edit distance = 3\n",
      "\n",
      "DP Table:\n",
      "    ##  #S  #a  #t  #u  #r  #d  #a  #y\n",
      "#  0  1  2  3  4  5  6  7  8\n",
      "S  1  0  1  2  3  4  5  6  7\n",
      "u  2  1  1  2  2  3  4  5  6\n",
      "n  3  2  2  2  3  3  4  5  6\n",
      "d  4  3  3  3  3  4  3  4  5\n",
      "a  5  4  3  4  4  4  4  3  4\n",
      "y  6  5  4  4  5  5  5  4  3\n",
      "\n",
      "One valid edit sequence:\n",
      "  ('match', ('S', 'S'))\n",
      "  ('ins', 'a')\n",
      "  ('ins', 't')\n",
      "  ('match', ('u', 'u'))\n",
      "  ('sub', ('n', 'r'))\n",
      "  ('match', ('d', 'd'))\n",
      "  ('match', ('a', 'a'))\n",
      "  ('match', ('y', 'y'))\n",
      "\n",
      "=== Model B: sub=2, ins=1, del=1 ===\n",
      "Edit distance = 4\n",
      "\n",
      "DP Table:\n",
      "    ##  #S  #a  #t  #u  #r  #d  #a  #y\n",
      "#  0  1  2  3  4  5  6  7  8\n",
      "S  1  0  1  2  3  4  5  6  7\n",
      "u  2  1  2  3  2  3  4  5  6\n",
      "n  3  2  3  4  3  4  5  6  7\n",
      "d  4  3  4  5  4  5  4  5  6\n",
      "a  5  4  3  4  5  6  5  4  5\n",
      "y  6  5  4  5  6  7  6  5  4\n",
      "\n",
      "One valid edit sequence:\n",
      "  ('match', ('S', 'S'))\n",
      "  ('ins', 'a')\n",
      "  ('ins', 't')\n",
      "  ('match', ('u', 'u'))\n",
      "  ('sub', ('n', 'r'))\n",
      "  ('match', ('d', 'd'))\n",
      "  ('match', ('a', 'a'))\n",
      "  ('match', ('y', 'y'))\n",
      "\n",
      "=== Reflection ===\n",
      "- Model A distance = 3, Model B distance = 4.\n",
      "- In Model A substitution was cheap, so we used it.\n",
      "- In Model B substitution was expensive, so insert/delete became equally good.\n",
      "- Insertions were necessary for 'a' and 't'.\n",
      "- Applications differ: spell check prefers Model A (subs common),\n",
      "  DNA alignment prefers Model B (insertions/deletions more realistic).\n"
     ]
    }
   ],
   "source": [
    "# Q4. Edit Distance between \"Sunday\" -> \"Saturday\"\n",
    "# Covers: Model A (sub=1, ins=1, del=1) and Model B (sub=2, ins=1, del=1)\n",
    "\n",
    "def edit_distance_and_ops(s, t, sub_cost=1, ins_cost=1, del_cost=1):\n",
    "    m, n = len(s), len(t)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    op = [[None]*(n+1) for _ in range(m+1)]\n",
    "\n",
    "    # Initialize DP base cases\n",
    "    for i in range(1, m+1):\n",
    "        dp[i][0] = i * del_cost\n",
    "        op[i][0] = ('del', s[i-1])\n",
    "    for j in range(1, n+1):\n",
    "        dp[0][j] = j * ins_cost\n",
    "        op[0][j] = ('ins', t[j-1])\n",
    "\n",
    "    # Fill DP table\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            cost_sub = dp[i-1][j-1] + (0 if s[i-1]==t[j-1] else sub_cost)\n",
    "            cost_del = dp[i-1][j] + del_cost\n",
    "            cost_ins = dp[i][j-1] + ins_cost\n",
    "            best = min(cost_sub, cost_del, cost_ins)\n",
    "            dp[i][j] = best\n",
    "            if best == cost_sub:\n",
    "                op[i][j] = ('match' if s[i-1]==t[j-1] else 'sub', (s[i-1], t[j-1]))\n",
    "            elif best == cost_del:\n",
    "                op[i][j] = ('del', s[i-1])\n",
    "            else:\n",
    "                op[i][j] = ('ins', t[j-1])\n",
    "\n",
    "    # Backtrace sequence\n",
    "    i, j = m, n\n",
    "    seq = []\n",
    "    while i > 0 or j > 0:\n",
    "        a = op[i][j]\n",
    "        if a is None:\n",
    "            break\n",
    "        if a[0] in ('match','sub'):\n",
    "            seq.append(a)\n",
    "            i -= 1; j -= 1\n",
    "        elif a[0]=='del':\n",
    "            seq.append(a)\n",
    "            i -= 1\n",
    "        else:\n",
    "            seq.append(a)\n",
    "            j -= 1\n",
    "    seq.reverse()\n",
    "    return dp, seq\n",
    "\n",
    "def print_dp_table(dp, s, t):\n",
    "    # Pretty print DP matrix\n",
    "    print(\"    \" + \"  \".join(\"#\"+c for c in \"#\"+t))\n",
    "    for i,row in enumerate(dp):\n",
    "        label = \"#\" if i==0 else s[i-1]\n",
    "        print(label, \" \".join(f\"{cell:2d}\" for cell in row))\n",
    "\n",
    "# --- Run for both models ---\n",
    "s, t = \"Sunday\", \"Saturday\"\n",
    "\n",
    "print(\"=== Model A: sub=1, ins=1, del=1 ===\")\n",
    "dpA, seqA = edit_distance_and_ops(s, t, sub_cost=1, ins_cost=1, del_cost=1)\n",
    "print(\"Edit distance =\", dpA[len(s)][len(t)])\n",
    "print(\"\\nDP Table:\")\n",
    "print_dp_table(dpA, s, t)\n",
    "print(\"\\nOne valid edit sequence:\")\n",
    "for step in seqA:\n",
    "    print(\" \", step)\n",
    "\n",
    "print(\"\\n=== Model B: sub=2, ins=1, del=1 ===\")\n",
    "dpB, seqB = edit_distance_and_ops(s, t, sub_cost=2, ins_cost=1, del_cost=1)\n",
    "print(\"Edit distance =\", dpB[len(s)][len(t)])\n",
    "print(\"\\nDP Table:\")\n",
    "print_dp_table(dpB, s, t)\n",
    "print(\"\\nOne valid edit sequence:\")\n",
    "for step in seqB:\n",
    "    print(\" \", step)\n",
    "\n",
    "# --- Reflection ---\n",
    "print(\"\\n=== Reflection ===\")\n",
    "print(\"- Model A distance = 3, Model B distance = 4.\")\n",
    "print(\"- In Model A substitution was cheap, so we used it.\")\n",
    "print(\"- In Model B substitution was expensive, so insert/delete became equally good.\")\n",
    "print(\"- Insertions were necessary for 'a' and 't'.\")\n",
    "print(\"- Applications differ: spell check prefers Model A (subs common),\")\n",
    "print(\"  DNA alignment prefers Model B (insertions/deletions more realistic).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b6565-5fe3-4bb5-b1b3-187cd01d919c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
